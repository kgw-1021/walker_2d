import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.monitor import Monitor
import numpy as np
import torch
import os

# ---------------------------------------------
# 1. ì„¤ì • ë° ì´ˆê¸°í™”
# ---------------------------------------------
log_dir = "./walker2d_curriculum_logs/"
os.makedirs(log_dir, exist_ok=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

# ---------------------------------------------
# 2. ì»¤ë¦¬í˜ëŸ¼ ì§€ì› í™˜ê²½ ë˜í¼
# ---------------------------------------------
class CommandAugmentedWalker(gym.Wrapper):
    """
    ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµì„ ì§€ì›í•˜ëŠ” Low-Level Agent í™˜ê²½ ë˜í¼
    - ë„˜ì–´ì§ ë°©ì§€ë¥¼ ìœ„í•´ ë³´ìƒ í•¨ìˆ˜ë¥¼ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
    """
    def __init__(self, env, command_low=0.0, final_command_high=3.0, init_command_high=0.5):
        super().__init__(env)
        self.command_low = command_low
        self.final_command_high = final_command_high # ìµœì¢… ëª©í‘œ (3.0 m/s)
        
        # [ì»¤ë¦¬í˜ëŸ¼] í˜„ì¬ ë‹¨ê³„ì˜ ìµœëŒ€ ì†ë„ ì œí•œ (ì´ˆê¸°ê°’: 0.5 m/s)
        self.curr_max_speed = init_command_high 
        
        self.current_command = 0.0
        
        # ê´€ì¸¡ ê³µê°„ í™•ì¥
        low = self.env.observation_space.low
        high = self.env.observation_space.high
        new_low = np.concatenate([low, np.array([command_low], dtype=np.float32)])
        new_high = np.concatenate([high, np.array([final_command_high], dtype=np.float32)])
        self.observation_space = spaces.Box(low=new_low, high=new_high, dtype=np.float32)

    def set_max_speed(self, speed):
        """ì™¸ë¶€ì—ì„œ ë‚œì´ë„(ìµœëŒ€ ì†ë„)ë¥¼ ì¡°ì ˆí•˜ê¸° ìœ„í•œ í•¨ìˆ˜"""
        self.curr_max_speed = min(speed, self.final_command_high)

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        
        # [ì»¤ë¦¬í˜ëŸ¼] í˜„ì¬ í—ˆìš©ëœ ìµœëŒ€ ì†ë„(curr_max_speed) ë‚´ì—ì„œ ëª©í‘œ ëœë¤ ì„¤ì •
        self.current_command = np.random.uniform(self.command_low, self.curr_max_speed)
        
        obs_aug = np.concatenate([obs, np.array([self.current_command], dtype=np.float32)])
        info["target_velocity"] = self.current_command
        return obs_aug, info

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        
        # Walker2dì˜ Obs[0]ì€ ëª¸í†µ Zì¢Œí‘œ(ë†’ì´)ì…ë‹ˆë‹¤.
        torso_height = obs[0]
        x_vel = info.get('x_velocity', obs[8])
        target_vel = self.current_command
        
        # ------------------- ë³´ìƒ ì„¤ê³„ ìˆ˜ì • (ë„˜ì–´ì§ ë°©ì§€ ê°•í™”) -------------------
        velocity_error = np.abs(target_vel - x_vel)
        
        # 1. ì¶”ì  ë³´ìƒ
        tracking_reward = -2.0 * velocity_error
        
        # 2. ë‹¬ì„± ë³´ë„ˆìŠ¤ 
        achievement_bonus = 1.5 if velocity_error < 0.3 else 0.0
        
        # 3. ìƒì¡´ ë³´ìƒ (ê¸°ì¡´ 0.05 -> 1.0 ìœ¼ë¡œ ê°•í™”)
        survival_reward = 1.0 
        
        # 4. ì œì–´ ë¹„ìš© (ê¸°ì¡´ -0.001 -> -0.1 ë¡œ ê°•í™”, ë¶ˆì•ˆì •í•œ ì›€ì§ì„ ë°©ì§€)
        ctrl_cost = -0.1 * np.sum(np.square(action)) 
        
        # 5. **ëª¸í†µ ë†’ì´ í˜ë„í‹° (ìƒˆë¡œ ì¶”ê°€)**: ë†’ì´ê°€ 0.8m ë¯¸ë§Œì¼ ë•Œ ê°•ë ¥í•œ í˜ë„í‹°
        height_penalty = 0.0
        MIN_HEIGHT_THRESHOLD = 0.8
        PENALTY_MULTIPLIER = 10.0
        
        if torso_height < MIN_HEIGHT_THRESHOLD:
            # ë†’ì´ê°€ ë‚®ì•„ì§ˆìˆ˜ë¡ í˜ë„í‹°ê°€ ê¸‰ê²©íˆ ì»¤ì§
            height_penalty = -PENALTY_MULTIPLIER * (MIN_HEIGHT_THRESHOLD - torso_height) 
            
        custom_reward = (
            tracking_reward + 
            achievement_bonus + 
            survival_reward + 
            ctrl_cost +
            height_penalty # ìƒˆë¡œìš´ ë†’ì´ í˜ë„í‹° ì¶”ê°€
        )

        obs_aug = np.concatenate([obs, np.array([self.current_command], dtype=np.float32)])
        
        info["target_velocity"] = target_vel
        info["velocity_error"] = velocity_error
        info["curr_max_speed"] = self.curr_max_speed # í˜„ì¬ ë‚œì´ë„ ì •ë³´ ê¸°ë¡
        
        return obs_aug, custom_reward, terminated, truncated, info

# ---------------------------------------------
# 3. ì»¤ë¦¬í˜ëŸ¼ ì½œë°± (í•µì‹¬ ë¡œì§)
# ---------------------------------------------
class CurriculumCallback(BaseCallback):
    """
    í•™ìŠµ ì„±ê³¼ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ë‹¤ê°€ ì˜í•˜ë©´ ë‚œì´ë„(ìµœëŒ€ ì†ë„)ë¥¼ ì˜¬ë¦¬ëŠ” ì½œë°±
    """
    def __init__(self, check_freq=5000, error_threshold=0.4, step_size=0.5, verbose=1):
        super().__init__(verbose)
        self.check_freq = check_freq
        self.error_threshold = error_threshold # ì´ ì˜¤ì°¨ë³´ë‹¤ ì¤„ì–´ë“¤ë©´ ë‚œì´ë„ ìƒìŠ¹
        self.step_size = step_size # í•œ ë²ˆì— ì˜¬ë¦´ ì†ë„ (m/s)
        self.error_buffer = []
        self.current_level = 0.5 # ì´ˆê¸° ë‚œì´ë„

    def _on_step(self) -> bool:
        # í˜„ì¬ ìŠ¤í…ì˜ velocity_error ì •ë³´ë¥¼ ìˆ˜ì§‘
        infos = self.locals.get("infos", [])
        for info in infos:
            if "velocity_error" in info:
                self.error_buffer.append(info["velocity_error"])
        
        # ì¼ì • ì£¼ê¸°ë§ˆë‹¤ í‰ê°€ ë° ë‚œì´ë„ ì¡°ì ˆ
        if self.n_calls % self.check_freq == 0:
            if len(self.error_buffer) > 0:
                mean_error = np.mean(self.error_buffer)
                
                if self.verbose > 0:
                    print(f"\n[Curriculum] Step {self.n_calls}: í‰ê·  ì†ë„ ì˜¤ì°¨ = {mean_error:.3f} (ëª©í‘œ: < {self.error_threshold})")
                    print(f"[Curriculum] í˜„ì¬ ìµœëŒ€ ì†ë„ ë ˆë²¨: {self.current_level:.1f} m/s")

                # ëª©í‘œ ë‹¬ì„± ì‹œ ë‚œì´ë„ ìƒìŠ¹
                if mean_error < self.error_threshold and self.current_level < 3.0:
                    self.current_level = min(self.current_level + self.step_size, 3.0)
                    
                    # í›ˆë ¨ í™˜ê²½(env)ì— ìƒˆë¡œìš´ ë‚œì´ë„ ì ìš©
                    # DummyVecEnv ë‚´ë¶€ì˜ ì›ë³¸ í™˜ê²½ë“¤ì— ì ‘ê·¼í•˜ì—¬ ê°’ ì„¤ì •
                    env = self.training_env
                    # Unwrapí•˜ì—¬ CommandAugmentedWalker ì°¾ê¸°
                    # (VecNormalize -> DummyVecEnv -> Monitor -> CommandAugmentedWalker ìˆœì„œ)
                    # ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•: get_attrì´ë‚˜ env_method ì‚¬ìš©
                    env.env_method("set_max_speed", self.current_level)
                    
                    print(f"ğŸ‰ ì„±ê³¼ ë‹¬ì„±! ë‚œì´ë„ ìƒìŠ¹ -> ìµœëŒ€ ì†ë„: {self.current_level:.1f} m/s ë¡œ ë³€ê²½ë¨.\n")
                
                # ë²„í¼ ì´ˆê¸°í™”
                self.error_buffer = []
        
        return True

# ---------------------------------------------
# 4. í™˜ê²½ ìƒì„± ë° ì‹¤í–‰
# ---------------------------------------------
def make_env():
    env = gym.make("Walker2d-v5") 
    # ì´ˆê¸° ë‚œì´ë„ 0.5ë¶€í„° ì‹œì‘, ìµœì¢… 3.0ê¹Œì§€
    env = CommandAugmentedWalker(env, command_low=0.0, final_command_high=3.0, init_command_high=0.5)
    env = Monitor(env)
    return env

if __name__ == "__main__":
    # í™˜ê²½ ìƒì„±
    env = DummyVecEnv([make_env for _ in range(1)])
    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)

    print(f"\nì»¤ë¦¬í˜ëŸ¼ SAC í•™ìŠµ ì‹œì‘ (Device: {device})")
    print("ì´ˆê¸° ëª©í‘œ ì†ë„ ë²”ìœ„: 0.0 ~ 0.5 m/s")
    
    model = SAC(
        "MlpPolicy",
        env,
        learning_rate=3e-4,
        buffer_size=1000000,
        batch_size=256,
        ent_coef='auto',
        verbose=1,
        tensorboard_log=log_dir + "tensorboard/",
        device=device,
    )

    # ì½œë°± ì„¤ì •
    # 1. ì»¤ë¦¬í˜ëŸ¼ ì½œë°±: 5000 ìŠ¤í…ë§ˆë‹¤ ê²€ì‚¬, ì˜¤ì°¨ê°€ 0.35 ë¯¸ë§Œì´ë©´ ë‚œì´ë„ 0.5ì”© ì¦ê°€
    curriculum_cb = CurriculumCallback(check_freq=5000, error_threshold=0.35, step_size=0.5)
    
    # 2. í‰ê°€ ì½œë°± (ê¸°ì¡´ ìœ ì§€)
    eval_env = DummyVecEnv([make_env for _ in range(1)])
    eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, clip_obs=10., training=False)
    
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=log_dir + "best_model/",
        log_path=log_dir + "eval/",
        eval_freq=10000,
        n_eval_episodes=5,
        deterministic=True
    )

    # í•™ìŠµ ì‹œì‘
    model.learn(total_timesteps=1000000, callback=[curriculum_cb, eval_callback], progress_bar=True)

    model.save(log_dir + "low_level_curriculum_final")
    env.save(log_dir + "vec_normalize.pkl")
    print("í•™ìŠµ ì™„ë£Œ.")

    # ---------------------------------------------
    # í…ŒìŠ¤íŠ¸ (ìµœì¢… ì„±ëŠ¥ í™•ì¸)
    # ---------------------------------------------
    print("\nìµœì¢… í…ŒìŠ¤íŠ¸ ì‹œì‘ (Max Speed 3.0)...")
    test_env_base = gym.make("Walker2d-v5", render_mode="human")
    # í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ìµœëŒ€ ë‚œì´ë„ë¡œ ì„¤ì •
    test_env = CommandAugmentedWalker(test_env_base, command_low=0.0, final_command_high=3.0, init_command_high=3.0)
    
    obs, info = test_env.reset()
    
    # ì •ê·œí™” í†µê³„ ë¡œë“œ
    obs_mean = env.obs_rms.mean
    obs_var = env.obs_rms.var
    epsilon = 1e-8

    for i in range(1000):
        # ìˆ˜ë™ ì •ê·œí™”
        obs_norm = (obs - obs_mean) / np.sqrt(obs_var + epsilon)
        obs_norm = np.clip(obs_norm, -10, 10)
        
        action, _ = model.predict(obs_norm, deterministic=True)
        obs, reward, terminated, truncated, info = test_env.step(action)
        
        print(f"Step {i}: ëª©í‘œ={info['target_velocity']:.2f}, í˜„ì¬={info['x_velocity']:.2f}")
        
        if terminated or truncated:
            obs, info = test_env.reset()

    test_env.close()